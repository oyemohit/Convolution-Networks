{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16 Implementation  \n",
    "  \n",
    "    \n",
    "      \n",
    "      \n",
    "  \n",
    "This Model replicates the model described in the [**VGG-16 paper.**](https://arxiv.org/pdf/1409.1556.pdf)  \n",
    "  \n",
    "  VGG-16 paper is based on the VGG's **ImageNet Challenge 2014 submission**, where their team secured the first and the second places in the localisation and classification tracks respectively.  \n",
    "  It's main contribution is a thorough evaluation of **networks of increasing depth** using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers.\n",
    "  \n",
    "However, the dataset used in our implementation is different, in which every image is of size **64 x 64 x 3**, so the first 3 stages of Convolution layers and 2 fully connected layers are implemented beacause of the small size of images.  \n",
    "  \n",
    "The model is implemented in **Keras with tensorflow backend.**  \n",
    "  \n",
    "Below Cell imports the required python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydotplus\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is stored in a **h5 file format**, so we extract the training & test data and Class labels from the h5 package.  \n",
    "  \n",
    "Following function extracts the data & labels and returns them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective:Loads the dataset & labels and returns Test & Train data\n",
    "#inputs:None\n",
    "#Outputs:Train & Test data and labels\n",
    "#invoked by:Main\n",
    "#invokes:None\n",
    "#approach:h5py library is used to load the dataset\n",
    "\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('data/train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # train set labels\n",
    "\n",
    "    test_dataset = h5py.File('data/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function converts the categorical class labels into one hot bit arrays so that deep learning algorithms can work on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective:One hot encoding of the data\n",
    "#inputs:labels<Y> and no. of classes<C>\n",
    "#Outputs:One hot encoded labels\n",
    "#invoked by:Main\n",
    "#invokes:None\n",
    "#approach:numpy functions are used to one hot encode\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data  \n",
    "  \n",
    "The dataset contains color images of hand signs at different angles and orientations which indicate integers from 0 to 5.  \n",
    "The dataset was taken from the [coursera.org](http://www.coursera.org) website.\n",
    "\n",
    "<img src=\"images/signs_data_kiank.png\" style=\"width:450px;height:250px;\">\n",
    "<caption><center> <font color='purple'>  **SIGNS dataset** </center></caption>\n",
    "Total images in Dataset:   **1200** images  \n",
    "Images in Training:        **1080** images  \n",
    "Images is Testing:         **120** images  \n",
    "Dimensions of an Image:    **64 x 64 x 3**  \n",
    "Number of classes: **6** Classes\n",
    "  \n",
    "  The dataset was taken from the coursera.org\n",
    "\n",
    "The images are normalized by dividing them with 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "#Objective:Calls the functions to load dataset and one hot encode labels (Main function)\n",
    "#inputs:N/A\n",
    "#Outputs:Prints the shapes of train and test data\n",
    "#invoked by:N/A\n",
    "#invokes:load_dataset() & convert_to_one_hot\n",
    "#approach:N/A\n",
    "\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below cell to look at a particular image from training set...(You can change value of i for corresponding image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBvqCKBQZ/H8M8IbKLRi9JAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21d2bf8e550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Objective:Displays any particular image from training set (change value of 'i' for different images)\n",
    "#inputs:N/A\n",
    "#Outputs:N/A\n",
    "#invoked by:N/A\n",
    "#invokes:N/A\n",
    "#approach:Uses Matplotlib functions to dislplay image\n",
    "\n",
    "i=80\n",
    "print('class: ',Y_train_orig[:,i])\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(X_train_orig[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Description:  \n",
    "\n",
    "Below Model uses **7 Convolution layers**. In every covulation layer:\n",
    "+ Filter size : **3 x 3**\n",
    "+ **No** Padding\n",
    "+ Strides : **1**\n",
    "+ All the parameters are **randomly initialized** with seed=0\n",
    "+ Activation function used : **Relu**\n",
    "  \n",
    "There are **3 MaxPool layers**,in which:\n",
    "+ Filter size : **2 x 2**\n",
    "+ Strides : **2**\n",
    "\n",
    "And **2 fully connected layers**:\n",
    "+ First fully connected layer has **1024 hidden units** and **Relu activation**\n",
    "+ second fully connected layer has **6 hidden units** and uses **Softmax activation**\n",
    "  \n",
    "The comvolution and Maxpooling layers are expanded accross 3 stages:\n",
    "+ First stage uses **64 filters**\n",
    "+ Second stage uses **128 filters**\n",
    "+ Third stage uses **256 filters**\n",
    "\n",
    "(Originally there are 5 stages in VGG16 Network but due to image size restrictions, our model is limited to 3 stages)  \n",
    "  \n",
    "After 3 Convolution stages, The images are **flattened** and feeded to fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective:Implements the VGG16 architecture\n",
    "#inputs:Shape of input image & No. of classes\n",
    "#Outputs:Returns the implemented CNN model\n",
    "#invoked by:N/A\n",
    "#invokes:N/A\n",
    "#approach:Uses Keras library functions to implement CNN model\n",
    "\n",
    "def VGG16(input_shape = (64, 64, 3), classes = 6):\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (3, 3), strides = (1, 1), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Conv2D(64, (3, 3), strides = (1, 1), name = 'conv2', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2, 2), strides=(2, 2),name='max_pool1')(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = Conv2D(128, (3, 3), strides = (1, 1), name = 'conv3', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Conv2D(128, (3, 3), strides = (1, 1), name = 'conv4', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2, 2), strides=(2, 2),name='max_pool2')(X)\n",
    "\n",
    "    # Stage 3 \n",
    "    X = Conv2D(256, (3, 3), strides = (1, 1), name = 'conv5', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Conv2D(256, (3, 3), strides = (1, 1), name = 'conv6', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Conv2D(256, (3, 3), strides = (1, 1), name = 'conv7', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2, 2), strides=(2, 2),name='max_pool3')(X)\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1024, activation='relu', name='fc1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='VGG16')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Cell creates the above described model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective:Invokes the VGG16() to create model\n",
    "#inputs:N/A\n",
    "#Outputs:N/A\n",
    "#invoked by:N/A\n",
    "#invokes:VGG16()\n",
    "#approach:N/A\n",
    "\n",
    "model = VGG16(input_shape = (64, 64, 3), classes = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the model we need to compile it using a optimizer in order to minimize a particular loss function.  \n",
    "   \n",
    "In our model:\n",
    "+ Optimizer used : **Adams Optimizer**\n",
    "+ Loss function : **Categorical Cross Entropy**\n",
    "\n",
    "Run following cell to compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective:Compiles the above implemented model on given parameters\n",
    "#inputs:Optimizer used, loss function used and Metrics to be displayed\n",
    "#Outputs:N/A\n",
    "#invoked by:N/A\n",
    "#invokes:N/A\n",
    "#approach:N/A\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model is ready to be trained on the traing set, Here we used:\n",
    "+ No. of iterations: **5**\n",
    "+ Size of mini batches: **16**  \n",
    "\n",
    "Run the following cell to train the model on 5 iterations with a batch size of 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 174s 161ms/step - loss: 13.1409 - acc: 0.1639\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 162s 150ms/step - loss: 13.4317 - acc: 0.1667\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 168s 155ms/step - loss: 13.4317 - acc: 0.1667\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 174s 161ms/step - loss: 13.4317 - acc: 0.1667\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 177s 164ms/step - loss: 13.4317 - acc: 0.1667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21d278ff320>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Objective:Fits the above compiled model on given training data\n",
    "#inputs:X_train, Y_train, no. of iterations and Batch size\n",
    "#Outputs:loss & accuracy after every iteration\n",
    "#invoked by:N/A\n",
    "#invokes:N/A\n",
    "#approach:N/A\n",
    "model.fit(X_train, Y_train, epochs = 5, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this model (trained on only five iterations) performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 6s 50ms/step\n",
      "Loss = 13.4317464828\n",
      "Test Accuracy = 0.166666667163\n"
     ]
    }
   ],
   "source": [
    "#Objective:Evaluates the above trained model on test data\n",
    "#inputs:X_test, Y_test\n",
    "#Outputs:Loss & Accuracy on test data\n",
    "#invoked by:N/A\n",
    "#invokes:N/A\n",
    "#approach:N/A\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a **summary** of the above model.    \n",
    "  \n",
    "  \n",
    "It shows the **dimensions of output** at every layer and the **no. of parameters** at every layer.  \n",
    "  \n",
    "  Total trainable Parameters in the model: **5,936,966**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 70, 70, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 68, 68, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 66, 66, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 66, 66, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 31, 31, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 29, 29, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 29, 29, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pool2 (MaxPooling2D)     (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 10, 10, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv7 (Conv2D)               (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pool3 (MaxPooling2D)     (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 5,936,966\n",
      "Trainable params: 5,936,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Objective:Displays the summary of the model (shape & No. of parameters at every layer)\n",
    "#inputs:N/A\n",
    "#Outputs:N/A\n",
    "#invoked by:N/A\n",
    "#invokes:N/A\n",
    "#approach:N/A\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OEpi5",
   "launcher_item_id": "jK9EQ"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
